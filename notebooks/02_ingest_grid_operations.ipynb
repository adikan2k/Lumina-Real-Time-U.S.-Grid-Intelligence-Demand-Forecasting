{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02 — Ingest Grid Operations (Hourly Demand & Generation)\n",
    "=========================================================\n",
    "Lumina Forecasting Hub\n",
    "\n",
    "Pulls hourly data from EIA API v2:\n",
    "  - electricity/rto/region-data  → actual demand, demand forecast, net generation, interchange\n",
    "  \n",
    "For each Balancing Authority in our config, we:\n",
    "  1. Paginate through the API (5000 rows/page)\n",
    "  2. Compute forecast error (MW and %)\n",
    "  3. Load into BigQuery fact_hourly_demand (partitioned by date, clustered by ba_code)\n",
    "\n",
    "Supports incremental loads — checks the max timestamp already in BQ and only pulls newer data.\n",
    "\n",
    "Usage in Colab:\n",
    "  1. Run 01_setup_bigquery_schema.py first\n",
    "  2. Set your EIA_API_KEY and GCP_PROJECT_ID below\n",
    "  3. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# ── Config ──────────────────────────────────────────────────────────\n",
    "EIA_API_KEY    = \"YOUR_EIA_API_KEY\"       # <-- UPDATE\n",
    "GCP_PROJECT_ID = \"YOUR_GCP_PROJECT_ID\"    # <-- UPDATE\n",
    "BQ_DATASET     = \"lumina\"\n",
    "EIA_BASE_URL   = \"https://api.eia.gov/v2\"\n",
    "\n",
    "BALANCING_AUTHORITIES = [\"PJM\", \"MISO\", \"ERCO\", \"CISO\", \"ISNE\", \"NYIS\", \"SWPP\", \"SOCO\", \"TVA\", \"DUK\"]\n",
    "BACKFILL_START = \"2024-01-01T00\"          # Hourly format for RTO data\n",
    "\n",
    "client = bigquery.Client(project=GCP_PROJECT_ID)\n",
    "print(f\"Connected to BigQuery: {GCP_PROJECT_ID}\")\n",
    "print(f\"Target BAs: {BALANCING_AUTHORITIES}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def fetch_eia_rto(ba_code, data_type, start, end=None, frequency=\"hourly\"):\n",
    "    \"\"\"\n",
    "    Fetch RTO data from EIA API v2 with automatic pagination.\n",
    "    \n",
    "    Args:\n",
    "        ba_code:   Balancing authority code (e.g., 'PJM')\n",
    "        data_type: 'D' for demand, 'DF' for demand forecast, 'NG' for net gen, 'TI' for interchange\n",
    "        start:     Start datetime string (e.g., '2024-01-01T00')\n",
    "        end:       End datetime string (optional)\n",
    "        frequency: 'hourly' (default) or 'local-hourly'\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame with columns [period, respondent, value, ...]\n",
    "    \"\"\"\n",
    "    route = f\"{EIA_BASE_URL}/electricity/rto/region-data/data/\"\n",
    "    \n",
    "    all_records = []\n",
    "    offset = 0\n",
    "    page_size = 5000\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"api_key\": EIA_API_KEY,\n",
    "            \"frequency\": frequency,\n",
    "            \"data[0]\": \"value\",\n",
    "            \"facets[respondent][]\": ba_code,\n",
    "            \"facets[type][]\": data_type,\n",
    "            \"sort[0][column]\": \"period\",\n",
    "            \"sort[0][direction]\": \"asc\",\n",
    "            \"offset\": offset,\n",
    "            \"length\": page_size,\n",
    "        }\n",
    "        if start:\n",
    "            params[\"start\"] = start\n",
    "        if end:\n",
    "            params[\"end\"] = end\n",
    "        \n",
    "        resp = requests.get(route, params=params)\n",
    "        resp.raise_for_status()\n",
    "        body = resp.json()\n",
    "        \n",
    "        data = body.get(\"response\", {}).get(\"data\", [])\n",
    "        total = int(body.get(\"response\", {}).get(\"total\", 0))\n",
    "        \n",
    "        if not data:\n",
    "            break\n",
    "        \n",
    "        all_records.extend(data)\n",
    "        offset += page_size\n",
    "        \n",
    "        print(f\"    [{ba_code}/{data_type}] fetched {len(all_records)}/{total} rows\", end=\"\\r\")\n",
    "        \n",
    "        if offset >= total:\n",
    "            break\n",
    "        \n",
    "        time.sleep(0.3)  # Rate limiting courtesy\n",
    "    \n",
    "    print(f\"    [{ba_code}/{data_type}] fetched {len(all_records)}/{total} rows — done\")\n",
    "    return pd.DataFrame(all_records)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_max_timestamp(ba_code):\n",
    "    \"\"\"\n",
    "    Returns the max timestamp_utc for a given BA already in BigQuery.\n",
    "    Returns BACKFILL_START if table is empty for that BA.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT MAX(timestamp_utc) AS max_ts\n",
    "    FROM `{GCP_PROJECT_ID}.{BQ_DATASET}.fact_hourly_demand`\n",
    "    WHERE ba_code = '{ba_code}'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = client.query(query).to_dataframe()\n",
    "        max_ts = result[\"max_ts\"].iloc[0]\n",
    "        if pd.isna(max_ts):\n",
    "            return BACKFILL_START\n",
    "        # Return 1 hour after max to avoid overlap\n",
    "        next_ts = pd.Timestamp(max_ts) + pd.Timedelta(hours=1)\n",
    "        return next_ts.strftime(\"%Y-%m-%dT%H\")\n",
    "    except Exception:\n",
    "        return BACKFILL_START"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def ingest_ba(ba_code, mode=\"incremental\"):\n",
    "    \"\"\"\n",
    "    Ingest all hourly data for one Balancing Authority.\n",
    "    \n",
    "    Fetches 4 series (demand, demand_forecast, net_generation, interchange),\n",
    "    merges them on timestamp, computes forecast error, and loads to BQ.\n",
    "    \"\"\"\n",
    "    if mode == \"incremental\":\n",
    "        start = get_max_timestamp(ba_code)\n",
    "    else:\n",
    "        start = BACKFILL_START\n",
    "    \n",
    "    end = datetime.utcnow().strftime(\"%Y-%m-%dT%H\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  BA: {ba_code} | Range: {start} → {end} | Mode: {mode}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Fetch each data type\n",
    "    data_types = {\n",
    "        \"D\":  \"demand_mw\",\n",
    "        \"DF\": \"demand_forecast_mw\",\n",
    "        \"NG\": \"net_generation_mw\",\n",
    "        \"TI\": \"interchange_mw\",\n",
    "    }\n",
    "    \n",
    "    dfs = {}\n",
    "    for dtype_code, col_name in data_types.items():\n",
    "        df = fetch_eia_rto(ba_code, dtype_code, start, end)\n",
    "        if df.empty:\n",
    "            print(f\"    WARNING: No data for {ba_code}/{dtype_code}\")\n",
    "            dfs[col_name] = pd.DataFrame(columns=[\"period\", col_name])\n",
    "            continue\n",
    "        df = df.rename(columns={\"value\": col_name})\n",
    "        df[col_name] = pd.to_numeric(df[col_name], errors=\"coerce\")\n",
    "        dfs[col_name] = df[[\"period\", col_name]].copy()\n",
    "    \n",
    "    # Merge all series on period\n",
    "    if all(d.empty for d in dfs.values()):\n",
    "        print(f\"    SKIP: No data returned for {ba_code}\")\n",
    "        return 0\n",
    "    \n",
    "    merged = None\n",
    "    for col_name, df in dfs.items():\n",
    "        if df.empty:\n",
    "            continue\n",
    "        if merged is None:\n",
    "            merged = df\n",
    "        else:\n",
    "            merged = merged.merge(df, on=\"period\", how=\"outer\")\n",
    "    \n",
    "    if merged is None or merged.empty:\n",
    "        print(f\"    SKIP: Empty merge for {ba_code}\")\n",
    "        return 0\n",
    "    \n",
    "    # Clean up\n",
    "    merged[\"ba_code\"] = ba_code\n",
    "    merged[\"timestamp_utc\"] = pd.to_datetime(merged[\"period\"], utc=True)\n",
    "    \n",
    "    # Compute forecast error\n",
    "    if \"demand_mw\" in merged.columns and \"demand_forecast_mw\" in merged.columns:\n",
    "        merged[\"forecast_error_mw\"] = merged[\"demand_mw\"] - merged[\"demand_forecast_mw\"]\n",
    "        merged[\"forecast_error_pct\"] = np.where(\n",
    "            merged[\"demand_forecast_mw\"] != 0,\n",
    "            (merged[\"forecast_error_mw\"] / merged[\"demand_forecast_mw\"]) * 100,\n",
    "            np.nan\n",
    "        )\n",
    "    else:\n",
    "        merged[\"forecast_error_mw\"] = np.nan\n",
    "        merged[\"forecast_error_pct\"] = np.nan\n",
    "    \n",
    "    # Select final columns\n",
    "    final_cols = [\n",
    "        \"timestamp_utc\", \"ba_code\",\n",
    "        \"demand_mw\", \"demand_forecast_mw\", \"net_generation_mw\", \"interchange_mw\",\n",
    "        \"forecast_error_mw\", \"forecast_error_pct\",\n",
    "    ]\n",
    "    for col in final_cols:\n",
    "        if col not in merged.columns:\n",
    "            merged[col] = np.nan\n",
    "    \n",
    "    result = merged[final_cols].copy()\n",
    "    result = result.dropna(subset=[\"timestamp_utc\"])\n",
    "    result = result.sort_values(\"timestamp_utc\").reset_index(drop=True)\n",
    "    \n",
    "    # Load to BigQuery\n",
    "    table_ref = f\"{GCP_PROJECT_ID}.{BQ_DATASET}.fact_hourly_demand\"\n",
    "    job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\")\n",
    "    \n",
    "    job = client.load_table_from_dataframe(result, table_ref, job_config=job_config)\n",
    "    job.result()\n",
    "    \n",
    "    print(f\"    Loaded {len(result)} rows to fact_hourly_demand\")\n",
    "    return len(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MODE = \"backfill\"  # Change to \"incremental\" after first run\n",
    "\n",
    "total_rows = 0\n",
    "results = {}\n",
    "\n",
    "for ba in BALANCING_AUTHORITIES:\n",
    "    try:\n",
    "        n = ingest_ba(ba, mode=MODE)\n",
    "        results[ba] = {\"status\": \"success\", \"rows\": n}\n",
    "        total_rows += n\n",
    "    except Exception as e:\n",
    "        results[ba] = {\"status\": \"error\", \"error\": str(e)}\n",
    "        print(f\"    ERROR for {ba}: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  INGESTION COMPLETE\")\n",
    "print(f\"  Total rows loaded: {total_rows:,}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(df_results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "quality_query = f\"\"\"\n",
    "WITH stats AS (\n",
    "    SELECT\n",
    "        ba_code,\n",
    "        COUNT(*) AS row_count,\n",
    "        MIN(timestamp_utc) AS earliest,\n",
    "        MAX(timestamp_utc) AS latest,\n",
    "        ROUND(AVG(demand_mw), 1) AS avg_demand_mw,\n",
    "        ROUND(AVG(ABS(forecast_error_pct)), 2) AS mean_abs_pct_error,\n",
    "        COUNTIF(demand_mw IS NULL) AS null_demand,\n",
    "        COUNTIF(demand_forecast_mw IS NULL) AS null_forecast,\n",
    "    FROM `{GCP_PROJECT_ID}.{BQ_DATASET}.fact_hourly_demand`\n",
    "    GROUP BY ba_code\n",
    ")\n",
    "SELECT * FROM stats ORDER BY row_count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_quality = client.query(quality_query).to_dataframe()\n",
    "print(\"\\n=== Data Quality Report: fact_hourly_demand ===\")\n",
    "print(df_quality.to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "viz_query = f\"\"\"\n",
    "SELECT\n",
    "    timestamp_utc,\n",
    "    ba_code,\n",
    "    demand_mw,\n",
    "    demand_forecast_mw,\n",
    "    forecast_error_pct\n",
    "FROM `{GCP_PROJECT_ID}.{BQ_DATASET}.fact_hourly_demand`\n",
    "WHERE ba_code = 'PJM'\n",
    "    AND timestamp_utc >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n",
    "ORDER BY timestamp_utc\n",
    "\"\"\"\n",
    "\n",
    "df_viz = client.query(viz_query).to_dataframe()\n",
    "\n",
    "if not df_viz.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "    \n",
    "    ax1.plot(df_viz[\"timestamp_utc\"], df_viz[\"demand_mw\"], label=\"Actual Demand\", linewidth=1.2)\n",
    "    ax1.plot(df_viz[\"timestamp_utc\"], df_viz[\"demand_forecast_mw\"], label=\"EIA Forecast\", linewidth=1.2, linestyle=\"--\")\n",
    "    ax1.set_ylabel(\"MW\")\n",
    "    ax1.set_title(\"PJM — Actual vs. Forecast Demand (Last 7 Days)\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.bar(df_viz[\"timestamp_utc\"], df_viz[\"forecast_error_pct\"], width=0.03, alpha=0.7, color=\"coral\")\n",
    "    ax2.axhline(0, color=\"black\", linewidth=0.5)\n",
    "    ax2.set_ylabel(\"Forecast Error %\")\n",
    "    ax2.set_xlabel(\"Time (UTC)\")\n",
    "    ax2.set_title(\"Forecast Error Distribution\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary stats\n",
    "    mape = df_viz[\"forecast_error_pct\"].abs().mean()\n",
    "    print(f\"\\nPJM 7-Day MAPE: {mape:.2f}%\")\n",
    "else:\n",
    "    print(\"No data available for visualization yet.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}